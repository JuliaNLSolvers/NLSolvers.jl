<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Optimization · NLSolvers.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">NLSolvers.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">NLSolvers.jl</a></li><li class="is-active"><a class="tocitem" href>Optimization</a><ul class="internal"><li><a class="tocitem" href="#Univariate-optimization"><span>Univariate optimization</span></a></li><li><a class="tocitem" href="#Multivariate-optimization"><span>Multivariate optimization</span></a></li><li><a class="tocitem" href="#Box-constrained-problems"><span>Box constrained problems</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Optimization</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Optimization</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaNLSolvers/NLSolvers.jl/blob/master/docs/src/optimization.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Optimization"><a class="docs-heading-anchor" href="#Optimization">Optimization</a><a id="Optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Optimization" title="Permalink"></a></h1><h2 id="Univariate-optimization"><a class="docs-heading-anchor" href="#Univariate-optimization">Univariate optimization</a><a id="Univariate-optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Univariate-optimization" title="Permalink"></a></h2><p>Brent&#39;s method for minimizing a scalar objective is implemented as the <code>BrentMin</code> method. To solve it, you need to provide an objective and bounds.</p><pre><code class="language-none">brent_f(x) = sin(x)
brent_scalar = ScalarObjective(; f = brent_f)
brent_prob = OptimizationProblem(brent_scalar, (π/2, 2*π))
solve(brent_prob, BrentMin(), OptimizationOptions())</code></pre><h2 id="Multivariate-optimization"><a class="docs-heading-anchor" href="#Multivariate-optimization">Multivariate optimization</a><a id="Multivariate-optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Multivariate-optimization" title="Permalink"></a></h2><p>Most applications of optimization software deals with multivariate optimization and there are many methods in the literature and in software that deals with these types of questions. Multivariate optimization can be unconstrained or constrained. We will show how to deal with these different cases below.</p><h3 id="Unconstrained-optimization"><a class="docs-heading-anchor" href="#Unconstrained-optimization">Unconstrained optimization</a><a id="Unconstrained-optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Unconstrained-optimization" title="Permalink"></a></h3><p>If there are no constraints present, there are a lot of methods to choose from. They typically require an objective and an initial point. So-called gradient based methods, or first order methods, require gradients as well, and the second order methods require Hessian information as well. Some methods can even exploit Hessian-vector products.</p><p>To show how all the pieces fit together, we can try to find a local minimizer of the Himmelblau function.</p><pre><code class="language-none">function himmelblau!(x)
    fx = (x[1]^2 + x[2] - 11)^2 + (x[1] + x[2]^2 - 7)^2
    return fx
end
function himmelblau_g!(∇f, x)
    ∇f[1] =
        4.0 * x[1]^3 + 4.0 * x[1] * x[2] - 44.0 * x[1] + 2.0 * x[1] + 2.0 * x[2]^2 - 14.0
    ∇f[2] =
        2.0 * x[1]^2 + 2.0 * x[2] - 22.0 + 4.0 * x[1] * x[2] + 4.0 * x[2]^3 - 28.0 * x[2]
    ∇f
end
function himmelblau_h!(∇²f, x)
    ∇²f[1, 1] = 12.0 * x[1]^2 + 4.0 * x[2] - 44.0 + 2.0
    ∇²f[1, 2] = 4.0 * x[1] + 4.0 * x[2]
    ∇²f[2, 1] = ∇²f[1, 2]
    ∇²f[2, 2] = 2.0 + 4.0 * x[1] + 12.0 * x[2]^2 - 28.0
    return ∇²f
end</code></pre><p>The preceding functions define the objective, its gradient, and its Hessian. Notice, that the functions have to have the following input</p><ul><li>objective functions require the current state <code>x</code>. It must return the objective value.</li><li>gradient functions require the gradient container <code>∇f</code> as the first argument and the state <code>x</code> as the second. It must return the gradient.</li><li>Hessian functions require the Hessian container <code>∇²f</code> as the first argument and the state <code>x</code> as the second. It must return the Hessian.</li></ul><p>Next, we wrap these up in the objective type</p><pre><code class="language-none">objective = ScalarObjective(
    f=himmelblau!,
    g=himmelblau_g!,
	h=himmelblau_h!,
    )</code></pre><p>The <code>ScalarObjective</code> signifies that for any given input the objective returns a single number. This is contrary to non-linear systems of equations for example. We should also set an initial point to search from.</p><pre><code class="language-none">x0 = [3.0, 1.0]</code></pre><p>Finally, we define the problem type.</p><pre><code class="language-none">prob = OptimizationProblem(objective)</code></pre><p>Then, we solve it.</p><pre><code class="language-none">julia&gt; results = solve(prob, x0, LineSearch(LBFGS()), OptimizationOptions())
Results of minimization

* Algorithm:
  Inverse LBFGS with backtracking (no interp)

* Candidate solution:
  Final objective value:    2.53e-25
  Final gradient norm:      3.87e-12

  Initial objective value:  1.00e+01
  Initial gradient norm:    1.80e+01

* Stopping criteria
  |x - x&#39;|              = 1.94e-08 &lt;= 0.00e+00 (false)
  |x - x&#39;|/|x|          = 5.37e-09 &lt;= 0.00e+00 (false)
  |f(x) - f(x&#39;)|        = 1.29e-14 &lt;= 0.00e+00 (false)
  |f(x) - f(x&#39;)|/|f(x)| = 1.00e+00 &lt;= 0.00e+00 (false)
  |g(x)|                = 3.87e-12 &lt;= 1.00e-08 (true)
  |g(x)|/|g(x₀)|        = 2.15e-13 &lt;= 0.00e+00 (false)

* Work counters
  Seconds run:   1.79e-05
  Iterations:    11</code></pre><p>Since we have Hessian information available, we can also use variants of Newton&#39;s method.</p><pre><code class="language-none">results = solve(prob, x0, TrustRegion(Newton()), OptimizationOptions())
julia&gt; results = solve(prob, x0, TrustRegion(Newton()), OptimizationOptions())
Results of minimization

* Algorithm:
  Newton&#39;s method with default linsolve with Trust Region (Newton, cholesky)

* Candidate solution:
  Final objective value:    7.10e-30
  Final gradient norm:      2.84e-14

  Initial objective value:  1.00e+01
  Initial gradient norm:    1.80e+01

* Stopping criteria
  |x - x&#39;|              = 2.73e-08 &lt;= 0.00e+00 (false)
  |x - x&#39;|/|x|          = 6.50e-09 &lt;= 0.00e+00 (false)
  |f(x) - f(x&#39;)|        = 3.01e-14 &lt;= 0.00e+00 (false)
  |f(x) - f(x&#39;)|/|f(x)| = 1.00e+00 &lt;= 0.00e+00 (false)
  |g(x)|                = 2.84e-14 &lt;= 1.00e-08 (true)
  |g(x)|/|g(x₀)|        = 1.58e-15 &lt;= 0.00e+00 (false)
  Δ                     = 7.63e+04 &lt;= 0.00e+00 (false)

* Work counters
  Seconds run:   6.39e-05
  Iterations:    9</code></pre><p>It is also possible to use methods that only use the objective to guide the search for an optimum. For example, it is possible to use the direct search method by Nelder and Mead.</p><pre><code class="language-none">julia&gt; results = solve(prob, x0, NelderMead(), OptimizationOptions())
Results of minimization

* Algorithm:
  Nelder-Mead

* Candidate solution:
  Final objective value:    0.00e+00

  Initial objective value:  0.00e+00

* Stopping criteria
  √(Σ(yᵢ-ȳ)²)/n         = 9.58e-09 &lt;= 1.00e-08 (true)

* Work counters
  Seconds run:   1.22e-02
  Iterations:    32</code></pre><p>We can also use a method based on sampling candidate solutions. Adaptive Particle Swarm is one such method. This method requires bounds on the state variable, so let us define a new optimization problem.</p><pre><code class="language-none">prob_bounds = OptimizationProblem(objective, ([0.0,0.0], [3.0,4.0]))</code></pre><p>Then, we can <code>solve</code> the problem using <code>ParticleSwarm</code>. We set the <code>maxiter</code> option, because the method has no real termination criteria, but will keep iterating until <code>maxiter</code> has been reached.</p><pre><code class="language-none">julia&gt; results = solve(prob_bounds, x0, ParticleSwarm(), OptimizationOptions(maxiter=38))
Results of minimization

* Algorithm:
  Adaptive Particle Swarm

* Candidate solution:
  Final objective value:    8.46e-14

  Initial objective value:  1.00e+01

* Stopping criteria

* Work counters
  Seconds run:   2.49e-04
  Iterations:    38</code></pre><h2 id="Box-constrained-problems"><a class="docs-heading-anchor" href="#Box-constrained-problems">Box constrained problems</a><a id="Box-constrained-problems-1"></a><a class="docs-heading-anchor-permalink" href="#Box-constrained-problems" title="Permalink"></a></h2><p>Box constraints, variable limits, and simple bounds are common names for variables where each individual parameter can have lower and upper bounds associated with them. We saw above how to enforce these bounds in the <code>ParticleSwarm</code> optimizer. There are other methods available. For example, <code>ActiveBounds</code> is a projected Newton&#39;s method. It was built for convex problem, so it can fail if the function is not locally convex</p><pre><code class="language-none">julia&gt; solve(prob_bounds, [0.0, 1.8], ActiveBox(), OptimizationOptions())
ERROR: PosDefException: matrix is not positive definite; Cholesky factorization failed.
...</code></pre><p>However, it can often work well if you specify a factorization with modifications if negative eigenvalues are detected, such as the one in PositiveFactorizations.jl.</p><pre><code class="language-none">julia&gt; solve(prob_bounds, [0.0, 1.8], ActiveBox(factorize=NLSolvers.positive_factorize), OptimizationOptions())
Results of minimization

* Algorithm:
  ActiveBox

* Candidate solution:
  Final objective value:    3.16e-30
  Final gradient norm:      2.84e-14
  Final projected gradient norm:  7.11e-15

  Initial objective value:  9.88e+01
  Initial gradient norm:    4.55e+01

* Stopping criteria
  |x - x&#39;|              = 1.02e-09 &lt;= 0.00e+00 (false)
  |x - x&#39;|/|x|          = 2.82e-10 &lt;= 0.00e+00 (false)
  |f(x) - f(x&#39;)|        = 1.42e-17 &lt;= 0.00e+00 (false)
  |f(x) - f(x&#39;)|/|f(x)| = 1.00e+00 &lt;= 0.00e+00 (false)
  |x - P(x - g(x))|     = 7.11e-15 &lt;= 1.00e-08 (true)
  |g(x)|                = 2.84e-14 &lt;= 1.00e-08 (true)
  |g(x)|/|g(x₀)|        = 6.25e-16 &lt;= 0.00e+00 (false)

* Work counters
  Seconds run:   1.26e-04
  Iterations:    12
</code></pre><p>If the solution happens to end up at the boundary it will be printed as part of the show method for the results. The following example will lead to a solution at the boundary.</p><pre><code class="language-none">prob_bounds = OptimizationProblem(objective, ([0.0,0.0], [2.5,2.8]))</code></pre><p>And has the following output.</p><pre><code class="language-none">julia&gt; solve(prob_bounds, [0.0, 1.8], ActiveBox(factorize=NLSolvers.positive_factorize), OptimizationOptions())
Results of minimization

* Algorithm:
  ActiveBox

* Candidate solution:
  Final objective value:    6.57e+00
  Final gradient norm:      2.39e+01
  Final projected gradient norm:  9.74e-11

  Initial objective value:  9.88e+01
  Initial gradient norm:    4.55e+01

* Stopping criteria
  |x - x&#39;|              = 1.90e-06 &lt;= 0.00e+00 (false)
  |x - x&#39;|/|x|          = 5.65e-07 &lt;= 0.00e+00 (false)
  |f(x) - f(x&#39;)|        = 8.07e-11 &lt;= 0.00e+00 (false)
  |f(x) - f(x&#39;)|/|f(x)| = 1.23e-11 &lt;= 0.00e+00 (false)
  |x - P(x - g(x))|     = 9.74e-11 &lt;= 1.00e-08 (true)
  |g(x)|                = 2.39e+01 &lt;= 1.00e-08 (false)
  |g(x)|/|g(x₀)|        = 5.26e-01 &lt;= 0.00e+00 (false)

  !!! Solution is at the boundary !!!

* Work counters
  Seconds run:   8.49e-05
  Iterations:    9</code></pre><p>Notice, that the final gradient norm is way above the default threshold, but the norm of the projected gradient is indeed small. There is also a message that the solution is at the boundary.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« NLSolvers.jl</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 19 December 2022 21:28">Monday 19 December 2022</span>. Using Julia version 1.8.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
